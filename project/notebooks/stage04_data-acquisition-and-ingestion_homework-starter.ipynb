{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211823d5-b329-46cf-947f-1ea2d8c695b8",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 04: Data Acquisition and Ingestion\n",
    "Name: Mingjia Jin\n",
    "Date: \n",
    "\n",
    "## Objectives\n",
    "- API ingestion with secrets in `.env`\n",
    "- Scrape a permitted public table\n",
    "- Validate and save raw data to `data/raw/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301f6250-9f70-470f-91b0-469742f79aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import requests # a Python library that allows you to make HTTP requests — like visiting a website, but with code.\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # Scrape Data\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "226d3b0c-b69b-4d51-ab27-10b9348562c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALPHAVANTAGE_API_KEY loaded? False\n"
     ]
    }
   ],
   "source": [
    "RAW = pathlib.Path('data/raw')\n",
    "RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "load_dotenv()\n",
    "print('ALPHAVANTAGE_API_KEY loaded?', bool(os.getenv('ALPHAVANTAGE_API_KEY'))) # yfinance do not need this one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1553742-0419-47b8-8741-bfaab7c3100e",
   "metadata": {},
   "source": [
    "## Helpers (use or modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f4c9e9-8cf8-47cd-bc12-a44418eb15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts():\n",
    "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S') # return current system time like 20250812-095532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c05978-3fd0-4b24-a0cb-a424b47ac10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent # Current Working Directory\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\" \n",
    "\n",
    "def save_csv(df: pd.DataFrame, prefix: str, source: str, ticker: str, ext: str = \"csv\") -> Path:\n",
    "    # Generate timestamp like '20250824-1052'\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    # Ensure the folder exists\n",
    "    DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "    # Construct the file path\n",
    "    path = DATA_RAW / f\"{prefix}_{source}_{ticker}_{ts}.{ext}\"\n",
    "    # Save the DataFrame\n",
    "    df.to_csv(path, index=True)  # index=True because Date is index\n",
    "    print(\"Saved:\", path)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad39ea0-4b62-4188-90db-d4deb6d0d777",
   "metadata": {},
   "source": [
    "Validate function requirements:\n",
    "\n",
    "| Requirement          | Meaning                                                  | Example                                                                                              |\n",
    "| -------------------- | -------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |\n",
    "| **Required columns** | The DataFrame must contain certain columns               | Stock data must include `Date, Open, High, Low, Close, Volume`                                       |\n",
    "| **NA counts**        | The number of missing values (NA/NaN) must be reasonable | For example, the `Volume` column may allow up to 20% missing values, but it cannot be entirely empty |\n",
    "| **Shape**            | The dataset must have enough rows, not just a few        | At least 100 rows of data are required; otherwise, the dataset is considered invalid                 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b845d9-5f13-49a4-935a-c02dc95c7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(df: pd.DataFrame, required, expected_dtypes):\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    \n",
    "    mismatch = {} # data type mismatch\n",
    "    for col, expected_type in expected_dtypes.items():\n",
    "        if col in df.columns:\n",
    "            actual = df[col].dtype\n",
    "            if not np.issubdtype(actual, expected_type):\n",
    "                mismatch[col] = str(actual)\n",
    "                \n",
    "    return {\n",
    "        'missing': missing,\n",
    "        'shape': df.shape,\n",
    "        'na_total': int(df.isna().sum().sum()),\n",
    "        'mismatched data type': mismatch,\n",
    "        'index is': df.index.name\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4222940e-e29c-4384-81bf-a228345dff0c",
   "metadata": {},
   "source": [
    "## Part 1 — API Pull (Required)\n",
    "`yfinance` here\n",
    "\n",
    "Choose an endpoint (e.g., Alpha Vantage or use `yfinance` fallback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22197dc8-7987-4427-93cb-be9b7cf1e7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Open        High         Low       Close  \\\n",
      "Date                                                                        \n",
      "2015-01-02 00:00:00-05:00   27.847500   27.860001   26.837500   27.332500   \n",
      "2015-01-05 00:00:00-05:00   27.072500   27.162500   26.352501   26.562500   \n",
      "2015-01-06 00:00:00-05:00   26.635000   26.857500   26.157499   26.565001   \n",
      "2015-01-07 00:00:00-05:00   26.799999   27.049999   26.674999   26.937500   \n",
      "2015-01-08 00:00:00-05:00   27.307501   28.037500   27.174999   27.972500   \n",
      "...                               ...         ...         ...         ...   \n",
      "2025-08-18 00:00:00-04:00  231.699997  233.119995  230.110001  230.889999   \n",
      "2025-08-19 00:00:00-04:00  231.279999  232.869995  229.350006  230.559998   \n",
      "2025-08-20 00:00:00-04:00  229.979996  230.470001  225.770004  226.009995   \n",
      "2025-08-21 00:00:00-04:00  226.270004  226.520004  223.779999  224.899994   \n",
      "2025-08-22 00:00:00-04:00  226.169998  229.089996  225.410004  227.759995   \n",
      "\n",
      "                            Adj Close     Volume  \n",
      "Date                                              \n",
      "2015-01-02 00:00:00-05:00   24.261044  212818400  \n",
      "2015-01-05 00:00:00-05:00   23.577572  257142000  \n",
      "2015-01-06 00:00:00-05:00   23.579794  263188400  \n",
      "2015-01-07 00:00:00-05:00   23.910433  160423600  \n",
      "2015-01-08 00:00:00-05:00   24.829124  237458000  \n",
      "...                               ...        ...  \n",
      "2025-08-18 00:00:00-04:00  230.889999   37476200  \n",
      "2025-08-19 00:00:00-04:00  230.559998   39402600  \n",
      "2025-08-20 00:00:00-04:00  226.009995   42263900  \n",
      "2025-08-21 00:00:00-04:00  224.899994   30621200  \n",
      "2025-08-22 00:00:00-04:00  227.759995   42445300  \n",
      "\n",
      "[2676 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "ticker = 'AAPL'\n",
    "START = \"2015-01-01\"\n",
    "END   = None\n",
    "\n",
    "USE_ALPHA = bool(os.getenv('ALPHAVANTAGE_API_KEY'))\n",
    "if USE_ALPHA:\n",
    "    url = 'https://www.alphavantage.co/query'\n",
    "    params = {'function':'TIME_SERIES_DAILY_ADJUSTED','symbol':SYMBOL,'outputsize':'compact','apikey':os.getenv('ALPHAVANTAGE_API_KEY')}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    key = [k for k in js if 'Time Series' in k][0]\n",
    "    df_api = pd.DataFrame(js[key]).T.reset_index().rename(columns={'index':'date','5. adjusted close':'adj_close'})[['date','adj_close']]\n",
    "    df_api['date'] = pd.to_datetime(df_api['date']); df_api['adj_close'] = pd.to_numeric(df_api['adj_close'])\n",
    "else:\n",
    "    import yfinance as yf\n",
    "    df_api = yf.Ticker(ticker).history(start=START, end=END, auto_adjust=False).drop(columns=[\"Dividends\", \"Stock Splits\"])\n",
    "    # df_api = yf.download(SYMBOL, period='3mo', interval='1d').reset_index()[['Date','Open','High','Low','Close','Adj Close','Volume']]\n",
    "\n",
    "print(df_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d8229e-2511-428d-8dee-e2dc7243989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'missing': ['Date'], 'shape': (2676, 6), 'na_total': 0, 'mismatched data type': {}, 'index is': 'Date'}\n"
     ]
    }
   ],
   "source": [
    "required_cols = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", 'Adj Close', \"Volume\"]\n",
    "expected_types = {\n",
    "    \"Date\": np.datetime64,\n",
    "    \"Open\": np.number,\n",
    "    \"High\": np.number,\n",
    "    \"Low\": np.number,\n",
    "    \"Close\": np.number,\n",
    "    \"Adj Close\": np.number,\n",
    "    \"Volume\": np.number\n",
    "}\n",
    "\n",
    "v_api = validate(df_api, required_cols, expected_types)\n",
    "print(v_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da50180c-bc5e-468c-b3c8-4656231219c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/fd/gitlocal/bootcamp_mingjia_jin/project/data/raw/api_yfinance_AAPL_20250824-1545.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/fd/gitlocal/bootcamp_mingjia_jin/project/data/raw/api_yfinance_AAPL_20250824-1545.csv')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save data\n",
    "save_csv(df_api, prefix=\"api\", source=\"yfinance\", ticker=\"AAPL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad507d36-f709-4def-b3ca-0a992a526f39",
   "metadata": {},
   "source": [
    "## Part 2 — Scrape a Public Table (Required)\n",
    "Replace `SCRAPE_URL` with a permitted page containing a simple table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6226980-97f7-4f35-8fdc-82296e35c8af",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3631983383.py, line 24)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m# v_scrape = validate(df_scrape, list(df_scrape.columns)); v_scrape\u001b[39m\n                                                                       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "SCRAPE_URL = 'https://example.com/markets-table'  # TODO: replace with permitted page\n",
    "headers = {'User-Agent':'AFE-Homework/1.0'}\n",
    "\n",
    "try:\n",
    "    resp = requests.get(SCRAPE_URL, headers=headers, timeout=30); resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    rows = [[c.get_text(strip=True) for c in tr.find_all(['th','td'])] for tr in soup.find_all('tr')]\n",
    "    header, *data = [r for r in rows if r]\n",
    "    df_scrape = pd.DataFrame(data, columns=header)\n",
    "\n",
    "# This fallback is not returning useful data — it’s just there to prevent the notebook from crashing.\n",
    "# So, ignore this.\n",
    "######################################################################################################\n",
    "# except Exception as e:\n",
    "#     print('Scrape failed, using inline demo table:', e)\n",
    "#     html = '<table><tr><th>Ticker</th><th>Price</th></tr><tr><td>AAA</td><td>101.2</td></tr></table>'\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "#     rows = [[c.get_text(strip=True) for c in tr.find_all(['th','td'])] for tr in soup.find_all('tr')]\n",
    "#     header, *data = [r for r in rows if r]\n",
    "#     df_scrape = pd.DataFrame(data, columns=header)\n",
    "\n",
    "# if 'Price' in df_scrape.columns:\n",
    "#     df_scrape['Price'] = pd.to_numeric(df_scrape['Price'], errors='coerce')\n",
    "# v_scrape = validate(df_scrape, list(df_scrape.columns)); v_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c9ed7c0-d406-4279-8664-0d14dd2c4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Target\n",
    "SCRAPE_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "headers = {\"User-Agent\": \"AFE-Homework/1.0\"}\n",
    "\n",
    "# --- Get and parse\n",
    "resp = requests.get(SCRAPE_URL, headers=headers, timeout=30)\n",
    "resp.raise_for_status() # It checks the HTTP response status code and raises an error if the request failed(404 or 500). \n",
    "soup = BeautifulSoup(resp.text, \"html.parser\") # resp.text： orginal web code; \"html.parser\": decoder, Python’s built-in HTML parser.\n",
    "\n",
    "# --- Find table\n",
    "table = soup.find(\"table\", {\"id\": \"constituents\"})\n",
    "\n",
    "# --- Extract rows\n",
    "rows = []\n",
    "for tr in table.find_all(\"tr\"):\n",
    "    cols = [td.get_text(strip=True) for td in tr.find_all([\"td\", \"th\"])]\n",
    "    if cols:\n",
    "        rows.append(cols)\n",
    "\n",
    "# --- Create DataFrame\n",
    "header = rows[0]\n",
    "data = rows[1:]\n",
    "df_scrape = pd.DataFrame(data, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8393412-cf9c-445a-896f-f657544ab790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Interest Rates</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Central Bank Rate</td>\n",
       "      <td>----%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Interest Rates  Value\n",
       "0    Central Bank Rate  ----%"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Target\n",
    "SCRAPE_URL = \"https://www.worldgovernmentbonds.com/country/united-states/\"\n",
    "headers = {\"User-Agent\": \"AFE-Homework/1.0\"}\n",
    "\n",
    "# --- Get and parse\n",
    "resp = requests.get(SCRAPE_URL, headers=headers, timeout=30)\n",
    "resp.raise_for_status()  # Raise error for 404, 500 etc\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")  # Convert HTML string into DOM tree\n",
    "\n",
    "# --- Find table\n",
    "# The table with U.S. bond yields has class 'table' and is the first one that matches bond layout\n",
    "# table = soup.find(\"table\", class_=\"table\")  # Or use soup.find_all(\"table\")[0] if unsure\n",
    "table = soup.find_all(\"table\")[0] \n",
    "\n",
    "# --- Extract rows\n",
    "rows = []\n",
    "for tr in table.find_all(\"tr\"):\n",
    "    cols = [td.get_text(strip=True) for td in tr.find_all([\"td\", \"th\"])] # Inside each row, finds all <td> and <th> cells.\n",
    "    if cols:\n",
    "        rows.append(cols)\n",
    "\n",
    "# --- Create DataFrame\n",
    "header = rows[0]\n",
    "data = rows[1:]\n",
    "df_scrape = pd.DataFrame(data, columns=header)\n",
    "\n",
    "df_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d5b4fd-3d83-4b7c-a937-dbce3bc6febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Setup\n",
    "headers = {'User-Agent': 'Mozilla/5.0 ... Safari/601.3.9'}\n",
    "url = 'https://finance.yahoo.com/most-active'\n",
    "\n",
    "# Fetch the page\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "# Each row of data is within elements having class 'simpTblRow'\n",
    "for item in soup.select('.simpTblRow'):\n",
    "    print(item)\n",
    "    print('------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dfb9161-25cd-4643-8752-782c52667ed3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m tables = soup.find_all(\u001b[33m\"\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Let's take the first table (usually 1-Year Treasury Yield or similar)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m table = \u001b[43mtables\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# --- Extract rows\u001b[39;00m\n\u001b[32m     17\u001b[39m rows = []\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# --- Target\n",
    "URL = \"https://www.federalreserve.gov/releases/h15/data.htm\"\n",
    "headers = {\"User-Agent\": \"AFE-Homework/1.0\"}\n",
    "\n",
    "# --- Get and parse\n",
    "resp = requests.get(URL, headers=headers)\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "\n",
    "# --- Find all tables (each rate has one table)\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "# Let's take the first table (usually 1-Year Treasury Yield or similar)\n",
    "table = tables[0]\n",
    "\n",
    "# --- Extract rows\n",
    "rows = []\n",
    "for tr in table.find_all(\"tr\"):\n",
    "    cols = [td.get_text(strip=True) for td in tr.find_all([\"td\", \"th\"])]\n",
    "    if cols:\n",
    "        rows.append(cols)\n",
    "\n",
    "print(rows)\n",
    "# --- Create DataFrame\n",
    "# header = rows[0]\n",
    "# data = rows[1:]\n",
    "# df = pd.DataFrame(data, columns=header)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ababbc58-ff8b-4a24-a696-69da06272376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional: save to CSV\n",
    "Path(\"data/raw\").mkdir(parents=True, exist_ok=True)\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "csv_path = f\"data/raw/scrape_us_bond_yields_{ts}.csv\"\n",
    "df_scrape.to_csv(csv_path, index=False)\n",
    "print(\"✅ Saved:\", csv_path)\n",
    "\n",
    "# --- Preview\n",
    "df_scrape.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e739cc-498f-41d7-809c-1cb87e3fcc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save\n",
    "Path(\"data/raw\").mkdir(parents=True, exist_ok=True)\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "df.to_csv(f\"data/raw/scrape_wiki_sp500_{ts}.csv\", index=False)\n",
    "print(\"✅ Saved:\", f\"data/raw/scrape_wiki_sp500_{ts}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c3348-56e8-4a5d-a115-66c0e260d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save raw data\n",
    "save_csv(df_api, prefix=\"api\", source=\"yahoo\", ident=\"AAPL\")\n",
    "\n",
    "save_csv(df_scrape, prefix='scrape', site='example', table='markets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1405d8fb-98cc-4cb7-864d-4ed9f221e605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324aa74d-0570-47c8-9d4d-fc05cbfec7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d51ab3-82a1-4d42-9434-ff8bf94993fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
